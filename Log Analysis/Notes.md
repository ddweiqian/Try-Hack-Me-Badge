# Why logs: Logs as Evidence of Historical Activity

## The Essence of Data: logs
Just as the rings in a tree reveal its life story—thicker rings for good years and thinner for challenging ones—a digital log documents the history of system activity. Both represent the fundamental principle of growth over time, serving as living records in their respective realms, whether physical or digital.

In the digital domain, every interaction with a computer system—such as authentication attempts, authorization grants, file accesses, network connections, and system errors—creates a digital footprint in the form of logs.

## Components of a Digital Log
Logs record events within a system, providing a comprehensive account of activities. These can include user logins, file accesses, system errors, network connections, and changes to data or system configurations.

<img src="https://github.com/user-attachments/assets/5d927f61-018d-4472-8bd3-5cd1e3992ce4" alt="Image description" width="400" height="300">

Although the specifics may vary by log type, a typical log entry contains:

1. A timestamp indicating when the event was logged

2. The name of the system or application generating the log entry

3. The type of event that occurred

4. Additional details, such as the user initiating the event or the IP address of the device involved

This information is typically stored in a log file, which aggregates entries of system activities over time. Due to the continuous and rapid pace of digital interactions, log files can grow significantly in size depending on the activities logged.

## The Power of Logs: Contextual Correlation
A single log entry may seem minor on its own. However, when log data is aggregated, analyzed, and cross-referenced with other information sources, it becomes a powerful investigative tool. Logs can provide answers to critical questions about an event, such as:

1. What happened?
   Answer : An adversary was confirmed to have accessed SwiftSpend Financial's GitLab instance.
2. When did it happen?
   Answer: Access started at 22:10 on Wednesday, September 8th, 2023.
3. Where did it happen?
   Answer: The event originated from a device with an IP address of 10.10.133.168 within the VPN Users' segment (10.10.133.0/24).
4. Who is responsible?
   Answer: Upon examining the network logs, it was observed that the device, identified by the User-Agent "Mozilla/5.0 (X11;     
   Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0", was allocated the IP address 10.10.133.168.
5. Were their actions successful?
   Answer: Yes, since an API Key was found to be publicly exposed on the GitLab instance. Moreover, the web proxy logs confirm that 
   the adversary device reached gitlab.swiftspend.finance and maintained access through their uploaded web shell.
6. What was the outcome of their actions?
    Answer: The adversary achieved remote code execution on gitlab.swiftspend.finance and performed post-exploitation activities.
   
The example above emphasises how logs are instrumental in piecing together a complete picture of an event, thereby enhancing our understanding and ability to respond effectively.
# What logs : Types, Formats and Standards

## Log Types

Different log types provide unique insights into a system's operation, performance, and security. Here, we'll focus on the most common log types, which cover around 80% of typical use cases.

### Common Log Types:
Application Logs: Include messages about specific applications, covering status updates, errors, warnings, etc.

Audit Logs: Capture activities related to operational procedures, essential for regulatory compliance.

Security Logs: Document security events such as logins, permissions changes, firewall activity, and more.

Server Logs: Encompass various logs generated by a server, including system, event, error, and access logs.

System Logs: Detail kernel activities, system errors, boot sequences, and hardware status.

Network Logs: Track network traffic, connections, and other network-related events.

Database Logs: Record activities within a database system, including queries and updates.

Web Server Logs: Capture requests processed by a web server, including URLs, response codes, etc.

### Importance of Log Analysis:

Understanding these log types, their formats, and standards is crucial for effective log analysis. This knowledge allows analysts to efficiently parse, interpret, and derive insights from log data, aiding in troubleshooting, performance optimization, incident response, and threat hunting.

## Log Standards

A log standard consists of guidelines or specifications that dictate how logs should be generated, transmitted, and stored. These standards can include specific log formats and also cover various aspects of logging, such as the events to be logged, secure transmission methods, and retention periods.

### Examples of Log Standards:

[Common Event Expression (CEE)](https://cee.mitre.org/): Developed by MITRE, this standard provides a common structure for log data, making it easier to generate, transmit, store, and analyze logs.

[OWASP Logging Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html): A guide for developers on building application logging mechanisms, particularly for security logging.

[Syslog Protocol](https://datatracker.ietf.org/doc/html/rfc5424): A standard for message logging that separates the software generating messages from the system storing them and the software reporting and analyzing them.

[NIST Special Publication 800-92](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf): Offers guidance on computer security log management.

[Azure Monitor Logs](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs): Provides guidelines for log monitoring on Microsoft Azure.

[Google Cloud Logging](https://cloud.google.com/logging/docs): Offers guidelines for logging on the Google Cloud Platform (GCP).

[Oracle Cloud Infrastructure Logging](https://docs.oracle.com/en-us/iaas/Content/Logging/Concepts/loggingoverview.htm): Provides guidelines for logging on the Oracle Cloud Infrastructure (OCI).

[Virginia Tech - Standard for Information Technology Logging](https://it.vt.edu/content/dam/it_vt_edu/policies/Standard_for_Information_Technology_Logging.pdf): A sample log review and compliance guideline.

Understanding these standards is crucial for effective log management, ensuring that logs are consistently and securely handled across different systems and platforms.

## Log Formats

A log format defines how data within a log file is structured and organized. It specifies the encoding method, the delimiters for each entry, and the fields included in each row. Log formats can vary significantly and generally fall into three main categories: Semi-structured, Structured, and Unstructured. Let's explore these categories with examples.

### Semi-structured Logs

These logs combine structured and unstructured data, with predictable components that allow for free-form text. Examples include:

Syslog Message Format: A widely adopted logging protocol for system and network logs.
![image](https://github.com/user-attachments/assets/0afe2ee4-d507-4c37-a739-d8939f673aae)

Windows Event Log (EVTX) Format: Proprietary Microsoft log for Windows systems.
![image](https://github.com/user-attachments/assets/cd14378f-86e4-44ee-995b-802f5983186a)

### Structured Logs: 

Following a strict and standardised format, these logs are conducive to parsing and analysis. Typical structured log formats include:

Field Delimited Formats: Comma-Separated Values (CSV) and Tab-Separated Values (TSV) are formats often used for tabular data.
![image](https://github.com/user-attachments/assets/0566d498-591f-49bd-b23b-0facb7d85c61)

JavaScript Object Notation (JSON): Known for its readability and compatibility with modern programming languages.
![image](https://github.com/user-attachments/assets/aa99c531-c9aa-4f7d-834d-5e5d712a8496)

W3C Extended Log Format (ELF): Defined by the World Wide Web Consortium (W3C), customizable for web server logging. It is typically used by Microsoft Internet Information Services (IIS) Web Server.
![image](https://github.com/user-attachments/assets/1639d5d0-c306-401f-9a85-d539e924ef47)

eXtensible Markup Language (XML): Flexible and customizable for creating standardized logging formats.
![image](https://github.com/user-attachments/assets/1b5ea5fe-172a-4c9a-a948-116eb5a8e659)

### Unstructured Logs: 

Comprising free-form text, these logs can be rich in context but may pose challenges in systematic parsing. Examples include:

NCSA Common Log Format (CLF): A standardized web server log format for client requests. It is typically used by the Apache HTTP Server by default.
![image](https://github.com/user-attachments/assets/372e8dec-6b81-4b12-8556-1a6b33896148)

NCSA Combined Log Format (Combined): An extension of CLF, adding fields like referrer and user agent. It is typically used by Nginx HTTP Server by default.
![image](https://github.com/user-attachments/assets/e112b997-41a7-485a-a294-bc1653687edb)



# How Log I: Collection, Management, and Centralisation

## Log Collection

Log collection is a crucial part of log analysis, involving the aggregation of logs from various sources like servers, network devices, software, and databases.

To accurately reflect a chronological sequence of events, it's essential to maintain precise system time during logging. Using the Network Time Protocol (NTP) helps achieve this synchronization and ensures the integrity of the log timelines.

This foundational step ensures that a security analyst has a comprehensive data set to review. Here's a simple step-by-step process, emphasizing the need to prioritize log collection based on significant information:

1. Identify Sources: List all potential log sources, such as servers, databases, applications, and network devices.

2. Choose a Log Collector: Select a log collector tool or software that aligns with your infrastructure, such as [rsyslog](https://www.rsyslog.com/doc/index.html)

3. Configure Collection Parameters: Enable time synchronization through NTP to maintain accurate timelines, set parameters for which events to log and their intervals, and prioritize based on importance.

4. Test Collection: Run a test to ensure logs are collected appropriately from all sources.

By following these steps, people can ensure effective log collection and accurate log analysis.

**IMPORTANT**: Please be aware that NTP-based time synchronisation may not be possible to replicate with the VM since it has no internet connectivity. However, when performing this in practice, using pool.ntp.org to find an NTP server is best. Time synchronisation can be performed automatically on Linux-based systems or manually initiated by executing. **ntpdate pool.ntp.org**.
![image](https://github.com/user-attachments/assets/5dc9ca9b-9dd3-443c-8199-9a1d081604cc)

## Log Management

Effective Log Management ensures that all collected logs are securely stored, systematically organized, and readily accessible. A hybrid approach, balancing comprehensive log retention with selective pruning, can provide an optimal solution.

After gathering your logs, managing them effectively is crucial. Follow these steps to achieve this:

1. **Storage**: Choose a secure storage solution, considering factors such as retention periods and accessibility.

2. **Organization: Classify logs by their source, type, or other criteria to facilitate easier access.

3. **Backup**: Regularly back up your logs to prevent data loss.

4. **Review**: Periodically review logs to ensure they are properly stored and categorized.

## Log Centralization

Centralization is essential for quick log access, thorough analysis, and rapid incident response. A unified system enables efficient log management with tools that offer real-time detection, automatic notifications, and seamless integration with incident management systems.

Here’s a streamlined process for achieving log centralization:

1. **Choose a Centralized System**: Opt for a platform that consolidates logs from all sources using SIEM, such as the Elastic Stack or Splunk.

2. **Integrate Sources**: Connect all your log sources to the centralized system.

3. **Set Up Monitoring**: Use tools that provide real-time monitoring and alerts for specific events.

4. **Integrate with Incident Management**: Ensure your centralized system can seamlessly integrate with incident management tools or protocols.


## Usecase Activity: Log Collection with rsyslog

This activity aims to introduce **rsyslog** and demonstrate how it can enhance the centralisation and management of logs. As part of the collection process, we will configure **rsyslog** to log all sshd messages to a specific file, such as **/var/log/websrv-02/rsyslog_sshd.log**. The steps below can be followed to achieve this:

1. **Open a Terminal**.
2. **Ensure rsyslog is Installed**: You can check if rsyslog is installed by running the command: **sudo systemctl status rsyslog**
3. **Create a Configuration File**: Use a text editor to create the following configuration file: **gedit /etc/rsyslog.d/98-websrv-02-sshd.conf**, **nano /etc/rsyslog.d/98-websrv-02-sshd.conf**, **vi /etc/rsyslog.d/98-websrv-02-sshd.conf**, or **vim /etc/rsyslog.d/98-websrv-02-sshd.conf**.
4. **Add the Configuration**: Add the following lines in **/etc/rsyslog.d/98-websrv-02-sshd.conf** to direct the sshd messages to the specific log file:
   
**$FileCreateMode 0644**
**:programname, isequal, "sshd" /var/log/websrv-02/rsyslog_sshd.log**

5. **Save and Close the Configuration File**.
6. **Restart rsyslog**: Apply the changes by restarting rsyslog with the command: **sudo systemctl restart rsyslog**
7. **Verify the Configuration**: You can verify the configuration works by initiating an SSH connection to localhost via **ssh localhost** or by checking the log file after a minute or two.

![image](https://github.com/user-attachments/assets/392203d2-2a33-448c-84dc-dccebc7da201)

**IMPORTANT**: If remote forwarding of logs is not configured, tools such as **scp** / **rsync**, among others, can be utilised for the manual collection of logs.

**More Reference**:

[How to Set Up Centralized Logging on Linux with rsyslog](https://betterstack.com/community/guides/logging/how-to-configure-centralised-rsyslog-server/)

[Centralized Log Collection Using Rsyslog](https://medium.com/curious-dev-grail/centralized-log-collection-using-rsyslog-1be6d6e9747f)

# How Log II: Storage, Retention, and Deletion

## Log Storage

Logs can be stored in various locations, such as the local system generating them, a centralized repository, or cloud-based storage.

### Factors Influencing Log Storage Location:
Logs can be stored in various locations, such as the local system that generates them, a centralised repository, or cloud-based storage.

The choice of storage location typically depends on multiple factors:

1. **Security Requirements**: Ensuring logs comply with organizational or regulatory security protocols.

2. **Accessibility Needs**: How quickly and by whom the logs need to be accessed.

3. **Storage Capacity**: The volume of logs generated may require significant storage space.

4. **Cost Considerations**: Budget constraints may influence the choice between cloud-based or local solutions.

5. **Compliance Regulations**: Industry-specific regulations governing log storage.

6. **Retention Policies**: Required retention time and ease of retrieval.

7. **Disaster Recovery Plans**: Ensuring log availability even in case of system failure.

## Log Retention

Recognizing that log storage is finite, it's crucial to balance retaining logs for potential future needs and storage costs. Understanding Hot, Warm, and Cold storage concepts aids in this decision-making:

1. **Hot Storage**: Most accessible logs from the past **3-6 months**, with near real-time query speeds.

2. **Warm Storage**: Logs from **six months to 2 years**, acting as a data lake, easily accessible but not as immediate as Hot storage.

3. **Cold Storage**: Archived or compressed logs from **2-5 years**, not easily accessible and used for retroactive analysis or scoping.

Managing log storage costs is critical for organizations, and selecting appropriate Hot, Warm, or Cold storage strategies can help keep these costs in check.

## Log Deletion

Carefully delete logs to avoid removing valuable ones. Always back up crucial log files before deletion.

### Importance of a Well-Defined Deletion Policy:
Log deletion must be performed carefully to avoid removing logs that could still be of value. The backup of log files, especially crucial ones, is necessary before deletion.

It is essential to have a well-defined deletion policy to ensure compliance with data protection laws and regulations. Log deletion helps to:

1. Maintain manageable log size for analysis.

2. Comply with privacy regulations, such as GDPR, which require the deletion of unnecessary data.

3. Keep storage costs balanced.

### Best Practices: Log Storage, Retention, and Deletion

1. Determine policies based on business needs and legal requirements.

2. Regularly review and update guidelines as conditions and regulations change.

3. Automate storage, retention, and deletion processes to ensure consistency and avoid human errors.

4. Encrypt sensitive logs to protect data.

5. Regular backups, especially before deletion.

### Usecase Activity: Log Management with logrotate
This activity aims to introduce **logrotate**, a tool that automates log file rotation, compression, and management, ensuring that log files are handled systematically. It allows automatic rotation, compression, and removal of log files. As an example, here's how we can set it up for **/var/log/websrv-02/rsyslog_sshd.log**:

1. **Create a Configuration File**: **sudo gedit /etc/logrotate.d/98-websrv-02_sshd.conf**, **sudo nano /etc/logrotate.d/98-websrv-02_sshd.conf**, **sudo vi /etc/logrotate.d/98-websrv-02_sshd.conf**, or **sudo vim /etc/logrotate.d/98-websrv-02_sshd.conf**

2. **Define Log Settings**:
![image](https://github.com/user-attachments/assets/c6ffd852-71de-4643-9a5c-039fd650f44c)

3. **Save and Close the file**.
4. Manual Execution: **sudo logrotate -f /etc/logrotate.d/98-websrv-02_sshd.conf**
![image](https://github.com/user-attachments/assets/c45ace27-3151-4e4e-8a8a-436dee0be346)

**More Reference**

[A Complete Guide to Managing Log Files with Logrotate](https://betterstack.com/community/guides/logging/how-to-manage-log-files-with-logrotate-on-ubuntu-20-04/)

[How to manage log files using logrotate](https://www.datadoghq.com/blog/log-file-control-with-logrotate/)

# Log Analysis : Process, tools, and techniques

Logs are not just records of past events; they serve as guiding compasses. They are invaluable resources that, when effectively leveraged, can enhance system diagnostics, cybersecurity, and regulatory compliance. Their role in documenting historical activity for systems or applications is crucial.

## Log Analysis Process

Log analysis involves several steps: Parsing, Normalization, Sorting, Classification, Enrichment, Correlation, Visualization, and Reporting. This can be achieved using various tools and techniques, from complex systems like [Splunk](https://dev.splunk.com/enterprise/tutorials) and [ELK](https://logz.io/learn/complete-guide-elk-stack/) to ad-hoc methods utilizing default command-line tools or open-source tools.

### Data Sources
Data sources refer to systems or applications configured to log system events or user activities using SIEM such as [Splunk](https://www.splunk.com/). These are the origins of logs.
([What is Splunk?](https://www.fortinet.com/resources/cyberglossary/what-is-splunk))

### Parsing
Parsing breaks down log data into more manageable and understandable components. Given that logs come in various formats, it's essential to parse them to extract valuable information. Practically, parsing can be done through Using [Splunk Engine](https://dataedge.ie/product/splunk-analytics-engine/) or tools such as [log parser](http://www.logparser.com/)

### Normalization
Normalization standardizes parsed data, bringing diverse log data into a consistent format. This process makes it easier to compare and analyze data from different sources, crucial in environments with multiple systems and applications. This can also be done in SIEM, such as [Splunk](https://dev.splunk.com/enterprise/tutorials), in general.

### Sorting
Sorting is vital for efficient data retrieval and pattern identification. Logs can be sorted by time into chronological order, source, event type, severity, and other parameters, making it easier to identify trends and anomalies signaling operational issues or security incidents.

### Classification
Classification assigns categories or labels to logs based on their characteristics, allowing for quick and easy filtering easily and focus on logs that matter most. This can be automated using machine learning to identify potential issues or threats efficiently. 

### Enrichment
Log enrichment adds context to logs, making them more meaningful and easier to analyze. It could involve adding geographical data, user details, threat intelligence, or data from other sources to provide a complete picture of an event.

### Correlation
This is the most important step in log analysis. Correlation links related records from different sources and identifies connections between log entries based on on attributes of logs such as severity, IP address, URLs. This helps detect patterns and trends, making it easier to understand complex relationships between various log events.

### Visualization
Visualization represents log data in graphical formats like charts, graphs, or heat maps, making it easier to recognize patterns, trends, and anomalies through reading a story. Visualization tools provide an intuitive way to interpret large volumes of log data.

### Reporting
Reporting summarizes log data into structured formats to provide insights, support decision-making, or meet compliance requirements. Effective reporting includes creating clear and concise log data summaries for stakeholders.

## Log Analysis Tools
For complex log analysis tasks, Security Information and Event Management (SIEM) tools such as Splunk or Elastic Search are used. 

In scenarios requiring immediate data analysis, tools like **cat, grep, sed, sort, uniq, and awk** on Linux-based systems, along with **sha256sum** for hashing log files. [EZ-Tools](https://ericzimmerman.github.io/#!index.md) and **Get-FileHash** on Windows-based systems, are valuable. Proper acquisition includes log file' **hashing during collection**  to ensure admissibility in court.

Therefore, it is imperative not only to log events but also to ensure their integrity, that they are analysed, and any lessons obtained from the logs be learned, as the safety and efficiency of an organisation can depend on them.

## Log Analysis Techniques

Log analysis techniques are methods used to interpret and derive insights from log data. They range from simple to complex and include:

1. **Pattern Recognition**: This involves identifying recurring sequences or trends in log data. It can detect regular system behaviour or identify unusual activities that may indicate a security threat.

2. **Anomaly Detection**:  Anomaly detection focuses on identifying data points that deviate from the expected pattern. It is crucial to spot potential issues or malicious activities early on.

3. **Correlation Analysis**: Correlating different log entries helps understand the relationship between various events. It can reveal causation and dependencies between system components and is vital in root cause analysis.

4. **Timeline Analysis**:  Analysing logs over time helps understand trends, seasonalities, and periodic behaviours. It can be essential for performance monitoring and forecasting system loads.

5. **Machine Learning and AI**: Leveraging machine learning models can automate and enhance various log analysis techniques, such as classification and enrichment. AI can provide predictive insights and help in automating responses to specific events.

6. **Visualization**: Representing log data through graphs and charts allows for intuitive understanding and quick insights. Visualisation can make complex data more accessible and assist in identifying key patterns and relationships.

7. **Statistical Analysis**: Using statistical methods to analyse log data can provide quantitative insights and help make data-driven decisions. Regression analysis and hypothesis testing can infer relationships and validate assumptions.

These techniques can be applied individually or in combination, depending on the specific requirements and complexity of the log analysis task. Understanding and using these techniques can significantly enhance the effectiveness of log analysis, leading to more informed decisions and robust security measures.

### Working with Logs: Usecase Application

Handling logs requires both comprehension and data manipulation. This includes two approaches:

**Unparsed Raw Logs**: Accessed directly via open-source [Log Viewer](https://github.com/sevdokimov/log-viewer) tools, suitable for quick inspections without preprocessing, which is ideal for quick inspections or preserving the original format.

**Parsed and Consolidated Logs**: Using Unix tools like **cat, grep, sed, sort, uniq**, and **awk** to create a standardized file, offering a clear and efficient data view through the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool, this consolidated file offers a clear and efficient view of the data, aiding in identifying patterns and issues.

These methods highlight the flexibility and importance of log analysis in system diagnostics and cybersecurity, ensuring the safety and efficiency of an organization.

### Unparsed Raw Log Files

When dealing with raw log files, you can access them directly through the Log Viewer tool by specifying the paths in the URL. Here's an example URL that includes multiple log files:
![image](https://github.com/user-attachments/assets/f330c410-c9b7-48cc-8a58-7e9dee87c0c9)

Paste this URL into your browser to view the unparsed raw log files using the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool.

**NOTE**: You can access the URL using the AttackBox or VM browser. However, please be aware that Firefox on the VM may take a few minutes to boot up.

### Parsed and Consolidated Log File
To create a parsed and consolidated log file, you can use a combination of Unix tools like **cat, grep, sed, sort, uniq**, and **awk**. Here's a step-by-step guide:

1. Use **awk** and **sed** to normalize the log entries to the desired format. For this example, we will sort by date and time:
![image](https://github.com/user-attachments/assets/2813b1c2-2a7c-415d-854d-62c8e522e396)

2. **Optional**: Use **grep** to filter specific entries:
![image](https://github.com/user-attachments/assets/a5517ddc-13fc-48b2-a38a-3566f73b0181)

3. Use **sort** to sort all the log entries by date and time:
![image](https://github.com/user-attachments/assets/193f38cc-5658-4265-88ec-d6c7615c372e)

4. Use **uniq** to remove duplicate entries:
![image](https://github.com/user-attachments/assets/f0914015-b720-4e89-8432-648871675d3a)

You can now access the parsed and consolidated log file through the [Log Viewer](https://github.com/sevdokimov/log-viewer) using the following URL:
![image](https://github.com/user-attachments/assets/f9d47c60-4b5d-40a0-90bc-b26cd54181d8)

![image](https://github.com/user-attachments/assets/34e98d3c-067e-4fdf-b5b0-9c5c3c421fdb)

**NOTE**: You can access the URL using the AttackBox or VM browser. However, please be aware that Firefox on the VM may take a few minutes to boot up

