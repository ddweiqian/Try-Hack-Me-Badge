# Why logs: Logs as Evidence of Historical Activity

## The Essence of Data: logs
Just as the rings in a tree reveal its life story—thicker rings for good years and thinner for challenging ones—a digital log documents the history of system activity. Both represent the fundamental principle of growth over time, serving as living records in their respective realms, whether physical or digital.

In the digital domain, every interaction with a computer system—such as authentication attempts, authorization grants, file accesses, network connections, and system errors—creates a digital footprint in the form of logs.

## Components of a Digital Log
Logs record events within a system, providing a comprehensive account of activities. These can include user logins, file accesses, system errors, network connections, and changes to data or system configurations.

<img src="https://github.com/user-attachments/assets/5d927f61-018d-4472-8bd3-5cd1e3992ce4" alt="Image description" width="400" height="300">

Although the specifics may vary by log type, a typical log entry contains:

1. A timestamp indicating when the event was logged

2. The name of the system or application generating the log entry

3. The type of event that occurred

4. Additional details, such as the user initiating the event or the IP address of the device involved

This information is typically stored in a log file, which aggregates entries of system activities over time. Due to the continuous and rapid pace of digital interactions, log files can grow significantly in size depending on the activities logged.

## The Power of Logs: Contextual Correlation
A single log entry may seem minor on its own. However, when log data is aggregated, analyzed, and cross-referenced with other information sources, it becomes a powerful investigative tool. Logs can provide answers to critical questions about an event, such as:

1. What happened?

2. When did it happen?

3. Where did it happen?

4. Who is responsible?

5. Were their actions successful?

6. What was the outcome of their actions?

# What logs : Types, Formats and Standards

## Log Types

Different log types provide unique insights into a system's operation, performance, and security. Here, we'll focus on the most common log types, which cover around 80% of typical use cases.

### Common Log Types:
Application Logs: Include messages about specific applications, covering status updates, errors, warnings, etc.

Audit Logs: Capture activities related to operational procedures, essential for regulatory compliance.

Security Logs: Document security events such as logins, permissions changes, firewall activity, and more.

Server Logs: Encompass various logs generated by a server, including system, event, error, and access logs.

System Logs: Detail kernel activities, system errors, boot sequences, and hardware status.

Network Logs: Track network traffic, connections, and other network-related events.

Database Logs: Record activities within a database system, including queries and updates.

Web Server Logs: Capture requests processed by a web server, including URLs, response codes, etc.

### Importance of Log Analysis:

Understanding these log types, their formats, and standards is crucial for effective log analysis. This knowledge allows analysts to efficiently parse, interpret, and derive insights from log data, aiding in troubleshooting, performance optimization, incident response, and threat hunting.

## Log Standards

A log standard consists of guidelines or specifications that dictate how logs should be generated, transmitted, and stored. These standards can include specific log formats and also cover various aspects of logging, such as the events to be logged, secure transmission methods, and retention periods.

### Examples of Log Standards:

[Common Event Expression (CEE)](https://cee.mitre.org/): Developed by MITRE, this standard provides a common structure for log data, making it easier to generate, transmit, store, and analyze logs.

[OWASP Logging Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html): A guide for developers on building application logging mechanisms, particularly for security logging.

[Syslog Protocol](https://datatracker.ietf.org/doc/html/rfc5424): A standard for message logging that separates the software generating messages from the system storing them and the software reporting and analyzing them.

[NIST Special Publication 800-92](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf): Offers guidance on computer security log management.

[Azure Monitor Logs](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs): Provides guidelines for log monitoring on Microsoft Azure.

[Google Cloud Logging](https://cloud.google.com/logging/docs): Offers guidelines for logging on the Google Cloud Platform (GCP).

[Oracle Cloud Infrastructure Logging](https://docs.oracle.com/en-us/iaas/Content/Logging/Concepts/loggingoverview.htm): Provides guidelines for logging on the Oracle Cloud Infrastructure (OCI).

[Virginia Tech - Standard for Information Technology Logging](https://it.vt.edu/content/dam/it_vt_edu/policies/Standard_for_Information_Technology_Logging.pdf): A sample log review and compliance guideline.

Understanding these standards is crucial for effective log management, ensuring that logs are consistently and securely handled across different systems and platforms.

## Log Formats

A log format defines how data within a log file is structured and organized. It specifies the encoding method, the delimiters for each entry, and the fields included in each row. Log formats can vary significantly and generally fall into three main categories: Semi-structured, Structured, and Unstructured. Let's explore these categories with examples.

### Semi-structured Logs

These logs combine structured and unstructured data, with predictable components that allow for free-form text. Examples include:

Syslog Message Format: A widely adopted logging protocol for system and network logs.
![image](https://github.com/user-attachments/assets/0afe2ee4-d507-4c37-a739-d8939f673aae)

Windows Event Log (EVTX) Format: Proprietary Microsoft log for Windows systems.
![image](https://github.com/user-attachments/assets/cd14378f-86e4-44ee-995b-802f5983186a)

### Structured Logs: 

Following a strict and standardised format, these logs are conducive to parsing and analysis. Typical structured log formats include:

Field Delimited Formats: Comma-Separated Values (CSV) and Tab-Separated Values (TSV) are formats often used for tabular data.
![image](https://github.com/user-attachments/assets/0566d498-591f-49bd-b23b-0facb7d85c61)

JavaScript Object Notation (JSON): Known for its readability and compatibility with modern programming languages.
![image](https://github.com/user-attachments/assets/aa99c531-c9aa-4f7d-834d-5e5d712a8496)

W3C Extended Log Format (ELF): Defined by the World Wide Web Consortium (W3C), customizable for web server logging. It is typically used by Microsoft Internet Information Services (IIS) Web Server.
![image](https://github.com/user-attachments/assets/1639d5d0-c306-401f-9a85-d539e924ef47)

eXtensible Markup Language (XML): Flexible and customizable for creating standardized logging formats.
![image](https://github.com/user-attachments/assets/1b5ea5fe-172a-4c9a-a948-116eb5a8e659)

### Unstructured Logs: 

Comprising free-form text, these logs can be rich in context but may pose challenges in systematic parsing. Examples include:

NCSA Common Log Format (CLF): A standardized web server log format for client requests. It is typically used by the Apache HTTP Server by default.
![image](https://github.com/user-attachments/assets/372e8dec-6b81-4b12-8556-1a6b33896148)

NCSA Combined Log Format (Combined): An extension of CLF, adding fields like referrer and user agent. It is typically used by Nginx HTTP Server by default.
![image](https://github.com/user-attachments/assets/e112b997-41a7-485a-a294-bc1653687edb)



# How Log I: Collection, Management, and Centralisation

## Log Collection

Log collection is a crucial part of log analysis, involving the aggregation of logs from various sources like servers, network devices, software, and databases.

To accurately reflect a chronological sequence of events, it's essential to maintain precise system time during logging. Using the Network Time Protocol (NTP) helps achieve this synchronization and ensures the integrity of the log timelines.

This foundational step ensures that a security analyst has a comprehensive data set to review. Here's a simple step-by-step process, emphasizing the need to prioritize log collection based on significant information:

1. Identify Sources: List all potential log sources, such as servers, databases, applications, and network devices.

2. Choose a Log Collector: Select a log collector tool or software that aligns with your infrastructure, such as [rsyslog](https://www.rsyslog.com/doc/index.html)

3. Configure Collection Parameters: Enable time synchronization through NTP to maintain accurate timelines, set parameters for which events to log and their intervals, and prioritize based on importance.

4. Test Collection: Run a test to ensure logs are collected appropriately from all sources.

By following these steps, people can ensure effective log collection and accurate log analysis.

## Log Management

Effective Log Management ensures that all collected logs are securely stored, systematically organized, and readily accessible. A hybrid approach, balancing comprehensive log retention with selective pruning, can provide an optimal solution.

After gathering your logs, managing them effectively is crucial. Follow these steps to achieve this:

1. Storage: Choose a secure storage solution, considering factors such as retention periods and accessibility.

2. Organization: Classify logs by their source, type, or other criteria to facilitate easier access.

3. Backup: Regularly back up your logs to prevent data loss.

4. Review: Periodically review logs to ensure they are properly stored and categorized.

## Log Centralization

Centralization is essential for quick log access, thorough analysis, and rapid incident response. A unified system enables efficient log management with tools that offer real-time detection, automatic notifications, and seamless integration with incident management systems.

Here’s a streamlined process for achieving log centralization:

1. Choose a Centralized System: Opt for a platform that consolidates logs from all sources using SIEM, such as the Elastic Stack or Splunk.

2. Integrate Sources: Connect all your log sources to the centralized system.

3. Set Up Monitoring: Use tools that provide real-time monitoring and alerts for specific events.

4. Integrate with Incident Management: Ensure your centralized system can seamlessly integrate with incident management tools or protocols.


## Usecase Activity: Log Collection with rsyslog
[How to Set Up Centralized Logging on Linux with rsyslog](https://betterstack.com/community/guides/logging/how-to-configure-centralised-rsyslog-server/)

[Centralized Log Collection Using Rsyslog](https://medium.com/curious-dev-grail/centralized-log-collection-using-rsyslog-1be6d6e9747f)

# How Log II: Storage, Retention, and Deletion

## Log Storage

Logs can be stored in various locations, such as the local system generating them, a centralized repository, or cloud-based storage.

### Factors Influencing Log Storage Location:

1. Security Requirements: Ensuring logs comply with organizational or regulatory security protocols.

2. Accessibility Needs: How quickly and by whom the logs need to be accessed.

3. Storage Capacity: The volume of logs generated may require significant storage space.

4. Cost Considerations: Budget constraints may influence the choice between cloud-based or local solutions.

5. Compliance Regulations: Industry-specific regulations governing log storage.

6. Retention Policies: Required retention time and ease of retrieval.

7. Disaster Recovery Plans: Ensuring log availability even in case of system failure.

## Log Retention

Recognizing that log storage is finite, it's crucial to balance retaining logs for potential future needs and storage costs. Understanding Hot, Warm, and Cold storage concepts aids in this decision-making:

1. Hot Storage: Most accessible logs from the past 3-6 months, with near real-time query speeds.

2. Warm Storage: Logs from six months to 2 years, acting as a data lake, easily accessible but not as immediate as Hot storage.

3. Cold Storage: Archived or compressed logs from 2-5 years, not easily accessible and used for retroactive analysis or scoping.

Managing log storage costs is critical for organizations, and selecting appropriate Hot, Warm, or Cold storage strategies can help keep these costs in check.

## Log Deletion

Carefully delete logs to avoid removing valuable ones. Always back up crucial log files before deletion.

### Importance of a Well-Defined Deletion Policy:

1. Maintain manageable log size for analysis.

2. Comply with privacy regulations, such as GDPR, which require the deletion of unnecessary data.

3. Keep storage costs balanced.

### Best Practices: Log Storage, Retention, and Deletion

1. Determine policies based on business needs and legal requirements.

2. Regularly review and update guidelines as conditions and regulations change.

3. Automate storage, retention, and deletion processes to ensure consistency and avoid human errors.

4. Encrypt sensitive logs to protect data.

5. Regular backups, especially before deletion.

### Usecase Activity: Log Management with logrotate

[A Complete Guide to Managing Log Files with Logrotate](https://betterstack.com/community/guides/logging/how-to-manage-log-files-with-logrotate-on-ubuntu-20-04/)

[How to manage log files using logrotate](https://www.datadoghq.com/blog/log-file-control-with-logrotate/)

# Log Analysis : Process, tools, and techniques

Logs are not just records of past events; they serve as guiding compasses. They are invaluable resources that, when effectively leveraged, can enhance system diagnostics, cybersecurity, and regulatory compliance. Their role in documenting historical activity for systems or applications is crucial.

## Log Analysis Process

Log analysis involves several steps: Parsing, Normalization, Sorting, Classification, Enrichment, Correlation, Visualization, and Reporting. This can be achieved using various tools and techniques, from complex systems like [Splunk](https://dev.splunk.com/enterprise/tutorials) and [ELK](https://logz.io/learn/complete-guide-elk-stack/) to ad-hoc methods utilizing default command-line tools or open-source tools.

### Data Sources
Data sources refer to systems or applications configured to log system events or user activities using SIEM such as [Splunk](https://www.splunk.com/). These are the origins of logs.
([What is Splunk?](https://www.fortinet.com/resources/cyberglossary/what-is-splunk))

### Parsing
Parsing breaks down log data into more manageable and understandable components. Given that logs come in various formats, it's essential to parse them to extract valuable information. Practically, parsing can be done through Using [Splunk Engine](https://dataedge.ie/product/splunk-analytics-engine/) or tools such as [log parser](http://www.logparser.com/)

### Normalization
Normalization standardizes parsed data, bringing diverse log data into a consistent format. This process makes it easier to compare and analyze data from different sources, crucial in environments with multiple systems and applications. This can also be done in SIEM, such as [Splunk](https://dev.splunk.com/enterprise/tutorials), in general.

### Sorting
Sorting is vital for efficient data retrieval and pattern identification. Logs can be sorted by time into chronological order, source, event type, severity, and other parameters, making it easier to identify trends and anomalies signaling operational issues or security incidents.

### Classification
Classification assigns categories or labels to logs based on their characteristics, allowing for quick and easy filtering easily and focus on logs that matter most. This can be automated using machine learning to identify potential issues or threats efficiently. 

### Enrichment
Log enrichment adds context to logs, making them more meaningful and easier to analyze. It could involve adding geographical data, user details, threat intelligence, or data from other sources to provide a complete picture of an event.

### Correlation
This is the most important step in log analysis. Correlation links related records from different sources and identifies connections between log entries based on on attributes of logs such as severity, IP address, URLs. This helps detect patterns and trends, making it easier to understand complex relationships between various log events.

### Visualization
Visualization represents log data in graphical formats like charts, graphs, or heat maps, making it easier to recognize patterns, trends, and anomalies through reading a story. Visualization tools provide an intuitive way to interpret large volumes of log data.

### Reporting
Reporting summarizes log data into structured formats to provide insights, support decision-making, or meet compliance requirements. Effective reporting includes creating clear and concise log data summaries for stakeholders.

## Log Analysis Tools
For complex log analysis tasks, Security Information and Event Management (SIEM) tools such as Splunk or Elastic Search are used. In scenarios requiring immediate data analysis, tools like cat, grep, sed, sort, uniq, and awk on Linux-based systems, and [EZ-Tools](https://ericzimmerman.github.io/#!index.md) and Get-FileHash on Windows-based systems, are valuable. Proper acquisition includes hashing log files during collection to ensure admissibility in court.

## Log Analysis Techniques

Log analysis techniques are methods used to interpret and derive insights from log data. They range from simple to complex and include:

1. Pattern Recognition: Identifying recurring sequences or trends.

2. Anomaly Detection: Spotting data points that deviate from the norm.

3. Correlation Analysis: Understanding relationships between events.

4. Timeline Analysis: Monitoring trends and behaviors over time.

5. Machine Learning and AI: Enhancing log analysis techniques like classification and enrichment.

6. Visualization: Using graphs and charts for intuitive data representation.

7. Statistical Analysis: Applying quantitative methods for data-driven decisions.

### Working with Logs: Usecase Application

Handling logs requires both comprehension and data manipulation. This includes two approaches:

Unparsed Raw Logs: Accessed directly via open-source [Log Viewer](https://github.com/sevdokimov/log-viewer) tools, suitable for quick inspections without preprocessing.

Parsed and Consolidated Logs: Using Unix tools like cat, grep, sed, sort, uniq, and awk to create a standardized file, offering a clear and efficient data view through the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool.

These methods highlight the flexibility and importance of log analysis in system diagnostics and cybersecurity, ensuring the safety and efficiency of an organization.
